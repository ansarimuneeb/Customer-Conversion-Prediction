{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyzkzGJerhpFWwkl3BgLOs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ansarimuneeb/Bankruptcy-Prediction/blob/main/Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya5aIK-ZwuD9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "# Upload the file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Read the file into a DataFrame\n",
        "if 'sales_data.csv' in uploaded:\n",
        "    # Read the file into a pandas DataFrame\n",
        "    data = pd.read_csv(io.BytesIO(uploaded['sales_data.csv']))\n",
        "    # Show the first few rows of the DataFrame\n",
        "    print(data.head())\n",
        "else:\n",
        "    # If the file isn't found, print an error message\n",
        "    print(\"File not found. Please check the filename and upload again.\")\n",
        "\n",
        "# Structure and summary of the data\n",
        "print(data.info())\n",
        "print(data.describe())\n",
        "\n",
        "# Check for missing values (NaN)\n",
        "print(data.isna().sum().sum())\n",
        "\n",
        "#Unique Values for each categorical variable\n",
        "unique_flag = data['flag'].unique()\n",
        "print(unique_flag)\n",
        "\n",
        "unique_gender = data['gender'].unique()\n",
        "print(unique_gender)\n",
        "\n",
        "unique_education = data['education'].unique()\n",
        "print(unique_education)\n",
        "\n",
        "unique_age = data['age'].unique()\n",
        "print(unique_age)\n",
        "\n",
        "unique_online = data['online'].unique()\n",
        "print(unique_online)\n",
        "\n",
        "unique_marriage = data['marriage'].unique()\n",
        "print(unique_marriage)\n",
        "\n",
        "unique_child = data['child'].unique()\n",
        "print(unique_child)\n",
        "\n",
        "unique_occupation = data['occupation'].unique()\n",
        "print(unique_occupation)\n",
        "\n",
        "unique_mortgage = data['mortgage'].unique()\n",
        "print(unique_mortgage)\n",
        "\n",
        "unique_house_owner = data['house_owner'].unique()\n",
        "print(unique_house_owner)\n",
        "\n",
        "unique_region = data['region'].unique()\n",
        "print(unique_region)\n",
        "\n",
        "unique_fam_income = data['fam_income'].unique()\n",
        "print(unique_fam_income)\n",
        "\n",
        "# Boxplot for house_value to identify outliers\n",
        "plt.boxplot(data['house_val'])\n",
        "plt.show()\n",
        "\n",
        "# Identifying and handling outliers in house_val\n",
        "print((data['house_val'] > 3000000).sum())\n",
        "print(((data['house_val'] > 3000000).sum() / len(data)) * 100)\n",
        "\n",
        "\n",
        "# Removing outliers\n",
        "data = data[data['house_val'] <= 3000000]\n",
        "\n",
        "# Handling weird naming in 'education'\n",
        "education_mapping = {\n",
        "    \"0. <HS\": \"<HS\",\n",
        "    \"1. HS\": \"HS\",\n",
        "    \"2. Some College\": \"College\",\n",
        "    \"3. Bach\": \"Bach\",\n",
        "    \"4. Grad\": \"Grad\",\n",
        "    \"\": np.nan # Convert empty strings to NaN\n",
        "}\n",
        "data['education'].replace(education_mapping, inplace=True)\n",
        "data.dropna(subset=['education'], inplace=True) # Remove rows with NaN in 'education'\n",
        "\n",
        "# Handling weird naming in 'age'\n",
        "age_mapping = {\n",
        "    \"1_Unk\": \"Unknown\",\n",
        "    \"2_<=25\": \"<=25\",\n",
        "    \"3_<=35\": \"<=35\",\n",
        "    \"4_<=45\": \"<=45\",\n",
        "    \"5_<=55\": \"<=55\",\n",
        "    \"6_<=65\": \"<=65\",\n",
        "    \"7_>65\": \">65\"\n",
        "}\n",
        "data['age'].replace(age_mapping, inplace=True)\n",
        "\n",
        "# Handling weird naming in 'mortgage'\n",
        "mortgage_mapping = {\n",
        "    \"1Low\": \"Low\",\n",
        "    \"2Med\": \"Med\",\n",
        "    \"3High\": \"High\"\n",
        "}\n",
        "data['mortgage'].replace(mortgage_mapping, inplace=True)\n",
        "\n",
        "# Handling missing values in 'house_owner' and 'marriage'\n",
        "data['house_owner'].replace(\"\", np.nan, inplace=True)\n",
        "data.dropna(subset=['house_owner'], inplace=True)\n",
        "\n",
        "data['marriage'].replace(\"\", \"Unknown\", inplace=True)\n",
        "\n",
        "# Convert individual variables to categorical types\n",
        "categorical_columns = ['flag', 'gender', 'education', 'age', 'online', 'marriage',\n",
        "                       'child', 'occupation', 'mortgage', 'house_owner', 'region', 'fam_income']\n",
        "for col in categorical_columns:\n",
        "    data[col] = data[col].astype('category')\n",
        "\n",
        "\n",
        "# Structure after conversions\n",
        "print(data.info())\n",
        "\n",
        "# Descriptive statistics for numeric variables\n",
        "numeric_stats = data.describe()\n",
        "\n",
        "# Frequency distribution for categorical variables\n",
        "categorical_stats = data.describe(include='category')\n",
        "\n",
        "print(\"Descriptive statistics for numeric variables:\")\n",
        "print(numeric_stats)\n",
        "\n",
        "print(\"\\nFrequency distribution for categorical variables:\")\n",
        "print(categorical_stats)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Using one-hot encoding for all categorical variables including 'flag'\n",
        "categorical_variables = ['flag', 'gender', 'education', 'age', 'online',\n",
        "                         'marriage', 'child', 'occupation', 'mortgage',\n",
        "                         'house_owner', 'region', 'fam_income']\n",
        "\n",
        "encoded_data = pd.get_dummies(data, columns=categorical_variables, drop_first=False)\n",
        "\n",
        "# Correlation matrix\n",
        "correlation_matrix = encoded_data.corr()\n",
        "\n",
        "# 'house_val' variable\n",
        "correlations_with_house_val = correlation_matrix['house_val'].sort_values(ascending=False)\n",
        "\n",
        "print(\"Correlations of all variables with 'house_val':\")\n",
        "print(correlations_with_house_val)\n",
        "\n",
        "# Heatmap for correlations with 'house_val'\n",
        "sorted_correlation_df = encoded_data[correlations_with_house_val.index].corr()\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(sorted_correlation_df[['house_val']], annot=True, cmap='coolwarm', center=0)\n",
        "plt.title('Correlation with House Value')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "encoded_data = data.copy()\n",
        "\n",
        "# Map binary categorical variables to numbers (for example, 'flag', 'online', etc.)\n",
        "binary_mappings = {\n",
        "    'Y': 1,\n",
        "    'N': 0\n",
        "}\n",
        "\n",
        "encoded_data['flag_numeric'] = encoded_data['flag'].map(binary_mappings)\n",
        "\n",
        "categorical_variables = ['gender', 'education', 'age', 'online',\n",
        "                         'marriage', 'child', 'occupation', 'mortgage',\n",
        "                         'house_owner', 'region', 'fam_income']\n",
        "\n",
        "encoded_data = pd.get_dummies(encoded_data, columns=categorical_variables, drop_first=True)\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = encoded_data.corr()\n",
        "\n",
        "correlation_pairs = correlation_matrix.where(np.tril(np.ones(correlation_matrix.shape), k=-1).astype(bool))\n",
        "\n",
        "# Stack the data and reset index\n",
        "correlation_stacked = correlation_pairs.stack().reset_index()\n",
        "\n",
        "# Name the columns\n",
        "correlation_stacked.columns = ['Variable 1', 'Variable 2', 'Correlation']\n",
        "\n",
        "# Calculate whether each correlation is positive or negative\n",
        "correlation_stacked['Positive'] = correlation_stacked['Correlation'] > 0\n",
        "\n",
        "# Sort by the absolute value of 'Correlation', descending\n",
        "correlation_stacked['AbsCorrelation'] = correlation_stacked['Correlation'].abs()\n",
        "top_correlations = correlation_stacked.sort_values('AbsCorrelation', ascending=False).head(15)\n",
        "\n",
        "# Drop the 'AbsCorrelation' column as it's no longer needed\n",
        "top_correlations = top_correlations.drop('AbsCorrelation', axis=1)\n",
        "\n",
        "# Display the top 15 correlations\n",
        "print(top_correlations)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the aesthetic style of the plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Visualisation 1: Distribution of house values\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data['house_val'], kde=True)\n",
        "plt.title('Distribution of House Values')\n",
        "plt.xlabel('House Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Visualisation 2: Count plot for education levels\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=data, x='education')\n",
        "plt.title('Count of Education Levels')\n",
        "plt.xlabel('Education')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Visualisation 3: Age distribution within each education level\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=data, x='education', y='house_val')\n",
        "plt.title('House Value across Education Levels')\n",
        "plt.xlabel('Education')\n",
        "plt.ylabel('House Value')\n",
        "plt.show()\n",
        "\n",
        "# Visualisation 4: Relationship between online activity and house value\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.violinplot(data=data, x='online', y='house_val')\n",
        "plt.title('House Value for Online and Offline Customers')\n",
        "plt.xlabel('Online')\n",
        "plt.ylabel('House Value')\n",
        "plt.show()\n",
        "\n",
        "# Visualisation 5: House value distribution by region\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=data, x='region', y='house_val')\n",
        "plt.title('Average House Value by Region')\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Average House Value')\n",
        "plt.show()\n",
        "\n",
        "# Visualisation 6: Family income vs. mortgage status\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=data, x='fam_income', hue='mortgage')\n",
        "plt.title('Family Income and Mortgage Status')\n",
        "plt.xlabel('Family Income')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Mortgage')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(data.info)\n",
        "print(encoded_data.info)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Map target variable 'flag' to numeric values\n",
        "data['flag_numeric'] = data['flag'].map({'Y': 1, 'N': 0})\n",
        "\n",
        "# Define features and target variable\n",
        "X = data.drop(['flag', 'flag_numeric'], axis=1)  # Drop the original flag and the encoded flag_numeric\n",
        "y = data['flag_numeric']\n",
        "\n",
        "# Define categorical features for encoding\n",
        "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# One-hot encode categorical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Random Forest Pipeline\n",
        "rf_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Logistic Regression Pipeline\n",
        "lr_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(solver='liblinear', random_state=42))\n",
        "])\n",
        "\n",
        "# Parameter grid for Random Forest\n",
        "param_grid_rf = {\n",
        "    'classifier__n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
        "    'classifier__max_depth': [None, 10, 20, 30],  # Maximum depth of the trees\n",
        "}\n",
        "\n",
        "# Parameter grid for Logistic Regression\n",
        "param_grid_lr = {\n",
        "    'classifier__C': [0.001, 0.01, 0.1, 1, 10],\n",
        "    'classifier__penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# GridSearch for Random Forest\n",
        "grid_search_rf = GridSearchCV(rf_pipeline, param_grid_rf, cv=5, n_jobs=-1, verbose=2)\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "print(f'Best parameters for Random Forest: {grid_search_rf.best_params_}')\n",
        "\n",
        "# GridSearch for Logistic Regression\n",
        "grid_search_lr = GridSearchCV(lr_pipeline, param_grid_lr, cv=5, n_jobs=-1, verbose=2)\n",
        "grid_search_lr.fit(X_train, y_train)\n",
        "print(f'Best parameters for Logistic Regression: {grid_search_lr.best_params_}')\n",
        "\n",
        "# Evaluate Random Forest\n",
        "y_pred_rf = grid_search_rf.predict(X_test)\n",
        "print(f'Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf)}')\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(confusion_matrix(y_test, y_pred_rf))\n",
        "\n",
        "# Evaluate Logistic Regression\n",
        "y_pred_lr = grid_search_lr.predict(X_test)\n",
        "print(f'Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_lr)}')\n",
        "print(classification_report(y_test, y_pred_lr))\n",
        "print(confusion_matrix(y_test, y_pred_lr))\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predict probabilities for the test set\n",
        "y_probs_rf = grid_search_rf.predict_proba(X_test)[:, 1]  # probabilities for the positive class\n",
        "\n",
        "# Compute ROC curve and area under the curve for Random Forest\n",
        "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_probs_rf)\n",
        "roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
        "\n",
        "# Plot ROC curve for Random Forest\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_rf, tpr_rf, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_rf:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic for Random Forest')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Predict probabilities for the test set\n",
        "y_probs_lr = grid_search_lr.predict_proba(X_test)[:, 1]  # probabilities for the positive class\n",
        "\n",
        "# Compute ROC curve and area under the curve for Logistic Regression\n",
        "fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, y_probs_lr)\n",
        "roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
        "\n",
        "# Plot ROC curve for Logistic Regression\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_lr, tpr_lr, color='green', lw=2, label=f'ROC curve (area = {roc_auc_lr:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic for Logistic Regression')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Models without parameter tuning\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Random Forest Pipeline without parameter tuning\n",
        "rf_model_simple = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))  # Using default parameters\n",
        "])\n",
        "# Train the Random Forest model\n",
        "rf_model_simple.fit(X_train, y_train)\n",
        "# Predictions with Random Forest\n",
        "y_pred_rf_simple = rf_model_simple.predict(X_test)\n",
        "\n",
        "# Evaluate Random Forest\n",
        "print('Random Forest Model without Parameter Tuning')\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred_rf_simple)}')\n",
        "print(classification_report(y_test, y_pred_rf_simple))\n",
        "print(confusion_matrix(y_test, y_pred_rf_simple))\n",
        "print('\\n')\n",
        "\n",
        "# Logistic Regression Pipeline without parameter tuning\n",
        "lr_model_simple = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(solver='liblinear', random_state=42))  # Using default parameters\n",
        "])\n",
        "# Train the Logistic Regression model\n",
        "lr_model_simple.fit(X_train, y_train)\n",
        "# Predictions with Logistic Regression\n",
        "y_pred_lr_simple = lr_model_simple.predict(X_test)\n",
        "# Evaluate Logistic Regression\n",
        "print('Logistic Regression Model without Parameter Tuning')\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred_lr_simple)}')\n",
        "print(classification_report(y_test, y_pred_lr_simple))\n",
        "print(confusion_matrix(y_test, y_pred_lr_simple))\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predict probabilities for the positive class with the Random Forest model\n",
        "y_probs_rf_simple = rf_model_simple.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC curve and area under the curve for Random Forest\n",
        "fpr_rf_simple, tpr_rf_simple, _ = roc_curve(y_test, y_probs_rf_simple)\n",
        "roc_auc_rf_simple = auc(fpr_rf_simple, tpr_rf_simple)\n",
        "\n",
        "# Plot ROC curve for Random Forest without parameter tuning\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_rf_simple, tpr_rf_simple, label=f'Random Forest (AUC = {roc_auc_rf_simple:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for reference\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for Random Forest Without Parameter Tuning')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Predict probabilities for the positive class with the Logistic Regression model\n",
        "y_probs_lr_simple = lr_model_simple.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC curve and area under the curve for Logistic Regression\n",
        "fpr_lr_simple, tpr_lr_simple, _ = roc_curve(y_test, y_probs_lr_simple)\n",
        "roc_auc_lr_simple = auc(fpr_lr_simple, tpr_lr_simple)\n",
        "\n",
        "# Plot ROC curve for Logistic Regression without parameter tuning\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_lr_simple, tpr_lr_simple, label=f'Logistic Regression (AUC = {roc_auc_lr_simple:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for reference\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for Logistic Regression Without Parameter Tuning')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Feature importance for Random Forest\n",
        "\n",
        "rf_model_fitted = grid_search_rf.best_estimator_['classifier']\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "rf_model_fitted = grid_search_rf.best_estimator_['classifier']\n",
        "\n",
        "# Get feature names for the one-hot encoded categorical features\n",
        "onehot_feature_names = grid_search_rf.best_estimator_.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(input_features=categorical_features)\n",
        "\n",
        "# Combine one-hot encoded feature names with non-categorical feature names\n",
        "non_categorical_features = X.select_dtypes(exclude=['object', 'category']).columns.tolist()\n",
        "all_feature_names = np.concatenate([onehot_feature_names, non_categorical_features])\n",
        "\n",
        "# Feature importances\n",
        "importances = rf_model_fitted.feature_importances_\n",
        "\n",
        "# Create DataFrame for easier interpretation\n",
        "importances_df = pd.DataFrame({\n",
        "    'Feature': all_feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "print(importances_df.sort_values(by='Importance', ascending=False))\n",
        "\n",
        "\n",
        "#Logistic Regression Coeffecients\n",
        "\n",
        "lr_model_fitted = grid_search_lr.best_estimator_['classifier']\n",
        "\n",
        "# Get feature names after one-hot encoding from the ColumnTransformer\n",
        "feature_names = grid_search_lr.best_estimator_.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
        "non_categorical_features = X.select_dtypes(exclude=['object', 'category']).columns.tolist()\n",
        "all_feature_names = np.concatenate([feature_names, non_categorical_features])\n",
        "\n",
        "# Coefficients\n",
        "coefficients = lr_model_fitted.coef_[0]\n",
        "\n",
        "# Create DataFrame for easier interpretation\n",
        "coefficients_df = pd.DataFrame({\n",
        "    'Feature': all_feature_names,\n",
        "    'Coefficient': coefficients\n",
        "})\n",
        "\n",
        "print(coefficients_df.sort_values(by='Coefficient', ascending=False))\n",
        "\n",
        "#Importances of Random Forest without tuning\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "importances = rf_model_simple.named_steps['classifier'].feature_importances_\n",
        "\n",
        "feature_names_transformed = rf_model_simple.named_steps['preprocessor'].get_feature_names_out()\n",
        "\n",
        "all_feature_names = feature_names_transformed\n",
        "\n",
        "# Create a DataFrame for feature importances for better readability\n",
        "importances_df = pd.DataFrame({\n",
        "    'Feature': all_feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(importances_df)\n",
        "\n",
        "\n",
        "#Coeffecients of Logistic Regression without Tuning\n",
        "\n",
        "# Access the preprocessor from the pipeline\n",
        "preprocessor = lr_model_simple.named_steps['preprocessor']\n",
        "\n",
        "feature_names_transformed = preprocessor.named_transformers_['cat'].get_feature_names_out()\n",
        "\n",
        "non_categorical_features = X.select_dtypes(exclude=['object', 'category']).columns\n",
        "all_feature_names = np.concatenate((feature_names_transformed, non_categorical_features), axis=None)\n",
        "\n",
        "# Retrieve the coefficients from the fitted logistic regression model within the pipeline\n",
        "coefficients = fitted_lr_model.coef_[0]\n",
        "\n",
        "# Create a DataFrame for easier interpretation\n",
        "coefficients_df = pd.DataFrame({\n",
        "    'Feature': all_feature_names,\n",
        "    'Coefficient': coefficients\n",
        "}).sort_values(by='Coefficient', ascending=False)\n",
        "\n",
        "print(coefficients_df)\n"
      ]
    }
  ]
}